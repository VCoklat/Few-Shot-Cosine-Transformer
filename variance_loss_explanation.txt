# Chapter: Variance Loss ($L_{var}$) for Prototype Separability

## 1. Introduction

### 1.1. The Goal: Separability
In classification tasks, the primary objective is to distinguish between classes. In the context of metric learning, this translates to maximizing the distance between the representations of different classes.
-   **Within-Class**: Samples of the same class should be close (Low Variance).
-   **Between-Class**: Prototypes of different classes should be far apart (High Variance / Separability).

### 1.2. The Role of $L_{var}$
The **Variance Loss** ($L_{var}$) explicitly targets the **Between-Class** objective. It penalizes the model if the prototypes of different classes are too similar to each other in terms of Cosine Similarity.

---

## 2. Mathematical Formulation

### 2.1. Definition
Let $\mathbf{P} \in \mathbb{R}^{N \times D}$ be the matrix of $N$ class prototypes, where each row $\mathbf{p}_i$ is L2-normalized ($\|\mathbf{p}_i\|_2 = 1$).

The Variance Loss is defined as the mean cosine similarity between all pairs of distinct prototypes:

$$ L_{var} = \frac{1}{N(N-1)} \sum_{i \neq j} \text{sim}(\mathbf{p}_i, \mathbf{p}_j) $$

Since the similarity matrix is symmetric, we can compute this efficiently using the upper triangular part:

$$ L_{var} = \frac{2}{N(N-1)} \sum_{i < j} \mathbf{p}_i \cdot \mathbf{p}_j $$

### 2.2. Interpretation
-   **Range**: Since $\mathbf{p}_i \cdot \mathbf{p}_j \in [-1, 1]$, the loss $L_{var} \in [-1, 1]$.
-   **Minimization**: We want to *minimize* this loss.
    -   Minimizing positive similarity $\to$ Pushing vectors to be orthogonal ($90^\circ$, sim=0).
    -   Minimizing further $\to$ Pushing vectors to be opposite ($180^\circ$, sim=-1).

---

## 3. Geometric Interpretation: The Tammes Problem

Minimizing $L_{var}$ is mathematically equivalent to solving a relaxation of the **Tammes Problem**: *How to pack $N$ points on a sphere such that the minimum distance between any two points is maximized.*

### 3.1. Optimal Configurations
The loss forces the prototypes to arrange themselves in a regular polyhedron configuration on the hypersphere.

| Number of Classes ($N$) | Optimal Configuration | Optimal Cosine Sim | Optimal $L_{var}$ |
| :--- | :--- | :--- | :--- |
| **2** | Opposite Poles (Line) | -1.0 | -1.0 |
| **3** (in 2D) | Equilateral Triangle | -0.5 | -0.5 |
| **4** (in 3D) | Regular Tetrahedron | -0.33 | -0.33 |
| **$N$** (in High Dim) | Simplex / Isotropic | $\approx 0$ | $\approx 0$ |

### 3.2. Visual Diagram

```mermaid
graph TD
    subgraph "High Loss (Bad)"
        A((P1)) --- B((P2))
        B --- C((P3))
        C --- A
        Note1[Clustered<br>Sim > 0]
    end
    
    subgraph "Optimal Loss (Good)"
        D((P1)) -.- E((P2))
        E -.- F((P3))
        F -.- D
        Note2[Equidistant<br>Sim < 0]
    end
    
    style A fill:#f99
    style B fill:#f99
    style C fill:#f99
    
    style D fill:#9f9
    style E fill:#9f9
    style F fill:#9f9
```

---

## 4. Why "Variance" Loss?

The name comes from the VICReg framework.
-   Technically, we are calculating the mean of the off-diagonal elements of the similarity matrix $\mathbf{S} = \mathbf{P}\mathbf{P}^T$.
-   In VICReg, the term is defined as a hinge loss on the standard deviation: $L = \max(0, 1 - \text{std}(Z))$.
-   In our **Prototype** adaptation, maximizing the "variance" of the prototype distribution over the sphere is equivalent to minimizing the pairwise similarity. If prototypes are clumped, their angular variance is low. If they are spread out, their angular variance is high.

---

## 5. Detailed Example Calculation

**Scenario**: 3-Way Classification ($N=3$).
**Prototypes**:
-   $\mathbf{p}_1 = [1, 0]$
-   $\mathbf{p}_2 = [0, 1]$
-   $\mathbf{p}_3 = [0.707, 0.707]$ (45 degrees between P1 and P2)

**Step 1: Compute Pairwise Similarities**
-   $S_{12} = 1*0 + 0*1 = 0.0$
-   $S_{13} = 1*0.707 + 0*0.707 = 0.707$
-   $S_{23} = 0*0.707 + 1*0.707 = 0.707$

**Step 2: Sum Off-Diagonals**
Sum = $0.0 + 0.707 + 0.707 = 1.414$

**Step 3: Normalize**
Number of pairs = $N(N-1)/2 = 3(2)/2 = 3$.
$L_{var} = 1.414 / 3 \approx 0.47$.

**Gradient Descent Effect**:
The gradient will push $\mathbf{p}_3$ away from $\mathbf{p}_1$ and $\mathbf{p}_2$.
Ideally, it would push $\mathbf{p}_3$ to $[-0.707, -0.707]$ (opposite direction), reducing the similarities to negative values.

---

## 6. Implementation Details

### 6.1. Code Snippet (`methods/optimal_few_shot.py`)

```python
# 1. Similarity Matrix
# [N, D] @ [D, N] -> [N, N]
sim_matrix = torch.mm(prototypes, prototypes.t())

# 2. Mask Diagonal
# We don't care that sim(p1, p1) == 1. We only care about p1 vs p2.
# triu(diagonal=1) selects strictly upper triangle.
mask = torch.triu(torch.ones_like(sim_matrix), diagonal=1).bool()

# 3. Extract and Average
similarities = sim_matrix[mask]
var_loss = similarities.mean()
```

### 6.2. Handling $N=1$
If the episode is 1-Way (rare, but possible in some setups), the loss is undefined (division by zero).
```python
if N > 1:
    # ... compute loss ...
else:
    var_loss = 0.0
```

---

## 7. Thesis Conclusion

The Variance Loss $L_{var}$ is a geometric regularizer. It acts as a repulsive force between class prototypes. By explicitly optimizing for this, we ensure that the feature space is utilized efficiently and that the "margin" between classes is maximized, which directly contributes to the robustness of the classifier against noisy query examples.
