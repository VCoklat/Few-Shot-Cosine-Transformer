# Chapter: Classification via Cosine Similarity

## 1. Introduction

### 1.1. The Final Decision
After the complex processes of feature extraction, projection, refinement, and prototype generation, the final step is **Classification**. This is where the model assigns a label (e.g., "Dog", "Cat", "Bird") to a query image.

### 1.2. The Metric Choice
In this architecture, we strictly use **Cosine Similarity** as the distance metric. This means we classify a query image based on the *angle* it makes with the class prototypes in the high-dimensional feature space, ignoring the magnitude (length) of the vectors.

---

## 2. Mathematical Formulation

### 2.1. Cosine Similarity Definition
Given a query vector $\mathbf{q} \in \mathbb{R}^D$ and a class prototype $\mathbf{p}_c \in \mathbb{R}^D$:

$$ \text{sim}(\mathbf{q}, \mathbf{p}_c) = \frac{\mathbf{q} \cdot \mathbf{p}_c}{\|\mathbf{q}\|_2 \|\mathbf{p}_c\|_2} $$

Since we enforce L2 normalization on both vectors ($\|\mathbf{q}\| = \|\mathbf{p}_c\| = 1$) in the previous steps, this simplifies to the dot product:

$$ \text{sim}(\mathbf{q}, \mathbf{p}_c) = \mathbf{q} \cdot \mathbf{p}_c = \sum_{i=1}^D q_i p_{c,i} $$

### 2.2. Probability Distribution (Softmax)
To convert these raw similarity scores into probabilities, we apply the Softmax function. However, raw cosine similarities are bounded in $[-1, 1]$, which is a very small range for the Softmax function (resulting in a "flat" distribution with low confidence).

To fix this, we introduce a learnable scaling factor $\alpha$ (Temperature):

$$ P(y=c | \mathbf{q}) = \frac{\exp(\alpha \cdot \text{sim}(\mathbf{q}, \mathbf{p}_c))}{\sum_{k=1}^N \exp(\alpha \cdot \text{sim}(\mathbf{q}, \mathbf{p}_k))} $$

Where:
-   $N$ is the number of classes (ways).
-   $\alpha$ is the inverse temperature parameter (typically initialized around 10.0).

---

## 3. Visual Explanation

### 3.1. Angular Classification

Imagine the feature space is a circle (2D hypersphere).

```mermaid
graph TD
    subgraph "Unit Hypersphere"
        Origin((0,0))
        
        P1[Prototype 1]
        P2[Prototype 2]
        Q[Query Image]
        
        Origin -- Vector P1 --> P1
        Origin -- Vector P2 --> P2
        Origin -- Vector Q --> Q
        
        linkStyle 0 stroke:red,stroke-width:2px;
        linkStyle 1 stroke:blue,stroke-width:2px;
        linkStyle 2 stroke:green,stroke-width:2px;
    end
    
    Note1[Angle θ1] --- Q
    Note1 --- P1
    
    Note2[Angle θ2] --- Q
    Note2 --- P2
```

-   **Vector P1 (Red)**: Prototype for Class 1.
-   **Vector P2 (Blue)**: Prototype for Class 2.
-   **Vector Q (Green)**: The Query Image.
-   **Decision**: Is Angle $\theta_1$ smaller than Angle $\theta_2$?
    -   If $\theta_1 < \theta_2$, then $\cos(\theta_1) > \cos(\theta_2)$.
    -   Classify Q as Class 1.

---

## 4. Implementation Details

### 4.1. Code Analysis
File: `methods/optimal_few_shot.py`

```python
# 1. Normalize Query Features
# Shape: [N_query_total, Dim]
query_norm = F.normalize(query_features, p=2, dim=1)

# 2. Normalize Prototypes (Redundant if done before, but safe)
# Shape: [N_way, Dim]
proto_norm = F.normalize(prototypes, p=2, dim=1)

# 3. Compute Similarity Matrix (Logits)
# Operation: Matrix Multiplication
# [N_query, Dim] x [Dim, N_way] = [N_query, N_way]
logits = torch.mm(query_norm, proto_norm.t()) * self.temperature
```

### 4.2. The Role of Temperature ($\alpha$)
In the code: `self.temperature = nn.Parameter(torch.tensor(10.0))`

**Why is it needed?**
-   **Without $\alpha$**: Similarities are in $[-1, 1]$.
    -   Sim(Correct) = 0.8, Sim(Incorrect) = 0.7.
    -   Softmax($e^{0.8}, e^{0.7}$) $\approx$ [0.52, 0.48].
    -   Result: The model is unsure, gradients are weak.
-   **With $\alpha=10$**:
    -   Scores become 8.0 and 7.0.
    -   Softmax($e^8, e^7$) $\approx$ [0.73, 0.27].
    -   Result: The model is confident, gradients are stronger.

The model *learns* the optimal $\alpha$. If the task is hard (classes overlap), it lowers $\alpha$ to express uncertainty. If easy, it raises $\alpha$.

---

## 5. Theoretical Comparison

### 5.1. Cosine vs. Euclidean
Standard Prototypical Networks use Euclidean Distance: $d(x,y) = \|x-y\|^2$.

| Feature | Euclidean Distance | Cosine Similarity |
| :--- | :--- | :--- |
| **Formula** | $\|x-y\|^2 = \|x\|^2 + \|y\|^2 - 2x \cdot y$ | $\frac{x \cdot y}{\|x\|\|y\|}$ |
| **Magnitude Sensitivity** | **High**. If $\|x\|$ is large, distance changes. | **None**. Scale invariant. |
| **High Dimensions** | Suffers from "Curse of Dimensionality". Distances concentrate. | More robust in high-dim spaces. |
| **Optimization Landscape** | Unbounded. Can explode. | Bounded $[-1, 1]$. Stable. |

**Thesis Argument**: In Few-Shot Learning, the *magnitude* of a feature vector often correlates with image contrast or brightness, which are irrelevant for classification. The *direction* encodes the semantic content. Therefore, Cosine Similarity is theoretically superior for this task.

---

## 6. Example Calculation

**Task**: 2-Way Classification (Cat vs. Dog).
**Dimension**: 3D.

**Vectors**:
-   Prototype Cat ($\mathbf{p}_c$): $[1, 0, 0]$
-   Prototype Dog ($\mathbf{p}_d$): $[0, 1, 0]$
-   Query Image ($\mathbf{q}$): $[0.6, 0.8, 0]$ (A dog-like image).

**Step 1: Normalization**
-   $\|\mathbf{p}_c\| = 1$.
-   $\|\mathbf{p}_d\| = 1$.
-   $\|\mathbf{q}\| = \sqrt{0.36 + 0.64} = 1$.

**Step 2: Dot Products**
-   Score(Cat) = $1*0.6 + 0*0.8 + 0*0 = 0.6$.
-   Score(Dog) = $0*0.6 + 1*0.8 + 0*0 = 0.8$.

**Step 3: Temperature Scaling ($\alpha=10$)**
-   Logit(Cat) = $6.0$.
-   Logit(Dog) = $8.0$.

**Step 4: Softmax Probability**
-   Denominator = $e^6 + e^8 \approx 403 + 2980 = 3383$.
-   P(Cat) = $403 / 3383 \approx 11.9\%$.
-   P(Dog) = $2980 / 3383 \approx 88.1\%$.

**Conclusion**: The model classifies the image as "Dog" with 88.1% confidence.
